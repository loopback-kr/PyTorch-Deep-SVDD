{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from train import TrainerDeepSVDD\n",
    "from preprocess import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 0, Loss: 160.687\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 1, Loss: 118.922\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 2, Loss: 85.630\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 3, Loss: 62.151\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 4, Loss: 46.533\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 5, Loss: 36.028\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 6, Loss: 28.815\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 7, Loss: 23.667\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 8, Loss: 19.928\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 9, Loss: 17.119\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 10, Loss: 14.952\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 11, Loss: 13.243\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 12, Loss: 11.860\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 13, Loss: 10.720\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 14, Loss: 9.774\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 15, Loss: 8.972\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 16, Loss: 8.286\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 17, Loss: 7.689\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 18, Loss: 7.157\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 19, Loss: 6.684\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 20, Loss: 6.252\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 21, Loss: 5.872\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 22, Loss: 5.533\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 23, Loss: 5.230\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 24, Loss: 4.956\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 25, Loss: 4.711\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 26, Loss: 4.492\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 27, Loss: 4.292\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 28, Loss: 4.107\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 29, Loss: 3.935\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 30, Loss: 3.778\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 31, Loss: 3.631\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 32, Loss: 3.495\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 33, Loss: 3.367\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 34, Loss: 3.246\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 35, Loss: 3.134\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 36, Loss: 3.027\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 37, Loss: 2.927\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 38, Loss: 2.836\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 39, Loss: 2.749\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 40, Loss: 2.668\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 41, Loss: 2.592\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 42, Loss: 2.520\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 43, Loss: 2.451\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 44, Loss: 2.385\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 45, Loss: 2.325\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 46, Loss: 2.267\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 47, Loss: 2.212\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 48, Loss: 2.157\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 49, Loss: 2.105\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 50, Loss: 2.076\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 51, Loss: 2.070\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 52, Loss: 2.065\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 53, Loss: 2.059\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 54, Loss: 2.056\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 55, Loss: 2.049\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 56, Loss: 2.044\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 57, Loss: 2.038\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 58, Loss: 2.032\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 59, Loss: 2.027\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 60, Loss: 2.022\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 61, Loss: 2.016\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 62, Loss: 2.011\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 63, Loss: 2.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 64, Loss: 2.000\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 65, Loss: 1.993\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 66, Loss: 1.988\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 67, Loss: 1.983\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 68, Loss: 1.976\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 69, Loss: 1.970\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 70, Loss: 1.963\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 71, Loss: 1.957\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 72, Loss: 1.951\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 73, Loss: 1.946\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 74, Loss: 1.939\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 75, Loss: 1.933\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 76, Loss: 1.925\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 77, Loss: 1.919\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 78, Loss: 1.913\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 79, Loss: 1.906\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 80, Loss: 1.900\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 81, Loss: 1.894\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 82, Loss: 1.887\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 83, Loss: 1.880\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 84, Loss: 1.874\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 85, Loss: 1.867\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 86, Loss: 1.860\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 87, Loss: 1.853\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 88, Loss: 1.846\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 89, Loss: 1.839\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 90, Loss: 1.832\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 91, Loss: 1.825\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 92, Loss: 1.818\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 93, Loss: 1.812\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 94, Loss: 1.805\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 95, Loss: 1.797\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 96, Loss: 1.790\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 97, Loss: 1.782\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 98, Loss: 1.774\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 99, Loss: 1.767\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 100, Loss: 1.761\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 101, Loss: 1.753\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 102, Loss: 1.745\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 103, Loss: 1.738\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 104, Loss: 1.731\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 105, Loss: 1.723\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 106, Loss: 1.716\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 107, Loss: 1.708\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 108, Loss: 1.701\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 109, Loss: 1.692\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 110, Loss: 1.686\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 111, Loss: 1.678\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 112, Loss: 1.670\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 113, Loss: 1.663\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 114, Loss: 1.655\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 115, Loss: 1.648\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 116, Loss: 1.640\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 117, Loss: 1.633\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 118, Loss: 1.626\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 119, Loss: 1.618\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 120, Loss: 1.609\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 121, Loss: 1.602\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 122, Loss: 1.595\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 123, Loss: 1.588\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 124, Loss: 1.580\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 125, Loss: 1.573\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 126, Loss: 1.565\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 127, Loss: 1.558\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 128, Loss: 1.550\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 129, Loss: 1.543\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 130, Loss: 1.535\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 131, Loss: 1.527\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 132, Loss: 1.520\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 133, Loss: 1.513\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 134, Loss: 1.505\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 135, Loss: 1.497\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 136, Loss: 1.490\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 137, Loss: 1.483\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 138, Loss: 1.474\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 139, Loss: 1.468\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 140, Loss: 1.461\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 141, Loss: 1.453\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 142, Loss: 1.445\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 143, Loss: 1.438\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 144, Loss: 1.431\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 145, Loss: 1.424\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 146, Loss: 1.416\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 147, Loss: 1.410\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 148, Loss: 1.402\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Pretraining Autoencoder... Epoch: 149, Loss: 1.394\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "\n",
    "    num_epochs=150\n",
    "    num_epochs_ae=150\n",
    "    patience=50\n",
    "    lr=1e-4\n",
    "    weight_decay=0.5e-6\n",
    "    weight_decay_ae=0.5e-3\n",
    "    lr_ae=1e-4\n",
    "    lr_milestones=[50]\n",
    "    batch_size=200\n",
    "    pretrain=True\n",
    "    latent_dim=32\n",
    "    normal_class=1\n",
    "    \n",
    "    \n",
    "args = Args()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = get_mnist(args)\n",
    "\n",
    "deep_SVDD = TrainerDeepSVDD(args, data, device)\n",
    "\n",
    "if args.pretrain:\n",
    "    deep_SVDD.pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 0, Loss: 0.739\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 1, Loss: 0.198\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 2, Loss: 0.053\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 3, Loss: 0.031\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 4, Loss: 0.024\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 5, Loss: 0.020\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 6, Loss: 0.018\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 7, Loss: 0.016\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 8, Loss: 0.014\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 9, Loss: 0.013\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 10, Loss: 0.012\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 11, Loss: 0.011\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 12, Loss: 0.010\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 13, Loss: 0.010\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 14, Loss: 0.009\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 15, Loss: 0.008\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 16, Loss: 0.008\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 17, Loss: 0.007\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 18, Loss: 0.007\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 19, Loss: 0.007\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 20, Loss: 0.006\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 21, Loss: 0.006\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 22, Loss: 0.006\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 23, Loss: 0.006\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 24, Loss: 0.005\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 25, Loss: 0.005\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 26, Loss: 0.005\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 27, Loss: 0.005\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 28, Loss: 0.005\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 29, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 30, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 31, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 32, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 33, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 34, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 35, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 36, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 37, Loss: 0.004\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 38, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 39, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 40, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 41, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 42, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 43, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 44, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 45, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 46, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 47, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 48, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 49, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 50, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 51, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 52, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 53, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 54, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 55, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 56, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 57, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 58, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 59, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 60, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 61, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 62, Loss: 0.003\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 63, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 64, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 65, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 66, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 67, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 68, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 69, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 70, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 71, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 72, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 73, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 74, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 75, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 76, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 77, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 78, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 79, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 80, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 81, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 82, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 83, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 84, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 85, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 86, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 87, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 88, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 89, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 90, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 91, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 92, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 93, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 94, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 95, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 96, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 97, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 98, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 99, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 100, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 101, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 102, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 103, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 104, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 105, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 106, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 107, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 108, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 109, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 110, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 111, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 112, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 113, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 114, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 115, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 116, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 117, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 118, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 119, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 120, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 121, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 122, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 123, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 124, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 125, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 126, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 127, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 128, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 129, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 130, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 131, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 132, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 133, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 134, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 135, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 136, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 137, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 138, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 139, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 140, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 141, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 142, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 143, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 144, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 145, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 146, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 147, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 148, Loss: 0.002\n",
      "6742/6742: [===============================>] - ETA 0.0s\n",
      "Training Deep SVDD... Epoch: 149, Loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "deep_SVDD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "ROC AUC score: 99.33\n"
     ]
    }
   ],
   "source": [
    "from test import eval\n",
    "\n",
    "labels, scores = eval(deep_SVDD.net, deep_SVDD.c, data[1], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "scores_in = scores[np.where(labels==0)[0]]\n",
    "scores_out = scores[np.where(labels==1)[0]]\n",
    "\n",
    "\n",
    "in_ = pd.DataFrame(scores_in, columns=['Inlier'])\n",
    "out_ = pd.DataFrame(scores_out, columns=['Outlier'])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "in_.plot.kde(ax=ax, legend=True, title='Outliers vs Inliers (Deep SVDD)')\n",
    "out_.plot.kde(ax=ax, legend=True)\n",
    "plt.xlim(-0.05, 0.08)\n",
    "ax.grid(axis='x')\n",
    "ax.grid(axis='y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
